{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1w9AYcq5R1L"
   },
   "source": [
    "# Big Data Management Project 2:\n",
    "## DESB GRAND CHALLENGE 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import unix_timestamp, regexp_extract, col, count, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gLNNqDSZ5R1O"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('BDM_Project2') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_NyMxyS5R1O"
   },
   "source": [
    "### Query 0\n",
    "Data Cleansing and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time 2.9444541931152344\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "# Defining the schema for faster reading of data\n",
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"surcharge\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Creating a single dataframe of all the trip_data files\n",
    "taxi_df_og = (\n",
    "    spark.readStream\n",
    "    .option(\"maxFilesPerTrigger\", 1) \n",
    "    .option(\"header\", False)\n",
    "    .schema(schema)\n",
    "    .csv(\"input/\")\n",
    ")\n",
    "\n",
    "# Removing the trips with 0 passengers\n",
    "# Transforming the data \n",
    "taxi_df = taxi_df_og.filter(\n",
    "    (regexp_extract(col(\"medallion\"), r\"^[a-fA-F0-9]{32}$\", 0) != \"\") &\n",
    "    (regexp_extract(col(\"hack_license\"), r\"^[a-fA-F0-9]{32}$\", 0) != \"\") &\n",
    "    (col(\"pickup_datetime\").isNotNull()) &\n",
    "    (col(\"dropoff_datetime\").isNotNull()) &               \n",
    "    (col(\"trip_distance\") > 0) &                    \n",
    "    (col(\"fare_amount\") > 0) &\n",
    "    (col(\"tip_amount\") >= 0)\n",
    ")\n",
    "\n",
    "# Convert timestamps to Unix format \n",
    "taxi_df = taxi_df.withColumn(\"pickup_ts\", unix_timestamp(\"pickup_datetime\")) \\\n",
    "    .withColumn(\"dropoff_ts\", unix_timestamp(\"dropoff_datetime\")) \\\n",
    "    .withColumn(\"duration\", col(\"dropoff_ts\") - col(\"pickup_ts\")) \\\n",
    "    .select(\"*\") \\\n",
    "    .dropna()  # Drop remaining null values\n",
    "\n",
    "# Start the streaming query with trigger(once=True) to process data once and stop\n",
    "query = (\n",
    "    taxi_df.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", \"output/preprocessed_data\")\n",
    "    .option(\"checkpointLocation\", \"output/checkpoint\")\n",
    "    .trigger(once=True)  \n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "print(\"Execution time\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+----------+----------+--------+\n",
      "|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount| pickup_ts|dropoff_ts|duration|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+----------+----------+--------+\n",
      "|64187531006B3D8B3...|871C25EC0B8BF4BC8...|2013-08-01 17:47:36|2013-08-01 19:05:40|             4683|         18.0|      -74.005295|      40.750935|       -73.783058|        40.64394|         CRD|       52.0|      0.0|    0.5|      10.5|         0.0|1375379256|1375383940|    4684|\n",
      "|B94FC976AA80935B3...|398ACB3ED20F01C7B...|2013-08-01 19:00:45|2013-08-01 19:05:40|              294|          0.7|      -73.975937|      40.749283|       -73.966896|       40.754253|         CRD|        5.0|      1.0|    0.5|       1.3|         0.0|1375383645|1375383940|     295|\n",
      "|F3E2FC48C5A14BACE...|ECB6E18FA26FB71E5...|2013-08-01 18:57:33|2013-08-01 19:05:40|              486|          0.8|        -73.9814|      40.763718|       -73.980293|       40.754337|         CRD|        6.5|      1.0|    0.5|       1.6|         0.0|1375383453|1375383940|     487|\n",
      "|E79712363B74EB213...|85CBC591C44A5E763...|2013-08-01 18:58:11|2013-08-01 19:05:41|              449|          1.8|      -74.006783|      40.728691|       -74.003693|       40.750854|         CRD|        8.5|      1.0|    0.5|       2.0|         0.0|1375383491|1375383941|     450|\n",
      "|FBB7B6B4099AD7F45...|58057A0DC7DC000BA...|2013-08-01 18:59:35|2013-08-01 19:05:41|              366|          1.5|      -73.965157|      40.759403|       -73.980194|       40.749165|         CRD|        6.5|      1.0|    0.5|       1.6|         0.0|1375383575|1375383941|     366|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+----------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_df = spark.read.parquet(\"output/preprocessed_data\")\n",
    "output_df.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = output_df.limit(2800000) # around 1gb of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original count: 90288423\n",
      "Filtered count: 2800000\n",
      "Rows filtered out: 87488423\n"
     ]
    }
   ],
   "source": [
    "# Impact of transformations\n",
    "original_count = output_df.count()\n",
    "filtered_count = sample_df.count()\n",
    "filtered_out_count = original_count - filtered_count\n",
    "\n",
    "print(f\"Original count: {original_count}\") \n",
    "print(f\"Filtered count: {filtered_count}\")\n",
    "print(f\"Rows filtered out: {filtered_out_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Cells for Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lat = 41.474937\n",
    "start_long = -74.913585\n",
    "cell_size = 0.044 # 500m to degrees for latitude (and longitude)\n",
    "\n",
    "def grid_cells_q1(point_long, point_lat):\n",
    "    \n",
    "    long = math.floor((point_long - start_long) / cell_size) + 1\n",
    "    lat = math.floor((start_lat - point_lat) / cell_size) + 1\n",
    "    \n",
    "    # Ensure the cell is within valid grid bounds (300x300)\n",
    "    if not (1 <= long <= 300 and 1 <= lat <= 300):\n",
    "        return None \n",
    "    \n",
    "    return float(f\"{long}.{lat}\") # Convert to X.X format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+----------+----------+--------+----------+--------+\n",
      "|medallion                       |hack_license                    |pickup_datetime    |dropoff_datetime   |trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|pickup_ts |dropoff_ts|duration|start_cell|end_cell|\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+----------+----------+--------+----------+--------+\n",
      "|64187531006B3D8B3831D91CD6AB3FD4|871C25EC0B8BF4BC8EF4CD3A79DD6BC8|2013-08-01 17:47:36|2013-08-01 19:05:40|4683             |18.0         |-74.005295      |40.750935      |-73.783058       |40.64394        |CRD         |52.0       |0.0      |0.5    |10.5      |0.0         |1375379256|1375383940|4684    |21.17     |26.19   |\n",
      "|B94FC976AA80935B3D978C872D8BDCA5|398ACB3ED20F01C7B015FD92199EF6FE|2013-08-01 19:00:45|2013-08-01 19:05:40|294              |0.7          |-73.975937      |40.749283      |-73.966896       |40.754253       |CRD         |5.0        |1.0      |0.5    |1.3       |0.0         |1375383645|1375383940|295     |22.17     |22.17   |\n",
      "|F3E2FC48C5A14BACE539216837A8AB86|ECB6E18FA26FB71E50B77E4E3265EE50|2013-08-01 18:57:33|2013-08-01 19:05:40|486              |0.8          |-73.9814        |40.763718      |-73.980293       |40.754337       |CRD         |6.5        |1.0      |0.5    |1.6       |0.0         |1375383453|1375383940|487     |22.17     |22.17   |\n",
      "|E79712363B74EB213BDE6F8FB1163FB4|85CBC591C44A5E763D4B9ECB11223732|2013-08-01 18:58:11|2013-08-01 19:05:41|449              |1.8          |-74.006783      |40.728691      |-74.003693       |40.750854       |CRD         |8.5        |1.0      |0.5    |2.0       |0.0         |1375383491|1375383941|450     |21.17     |21.17   |\n",
      "|FBB7B6B4099AD7F45A8AAC9A4A40A5E6|58057A0DC7DC000BA6A66DA13C87DD5D|2013-08-01 18:59:35|2013-08-01 19:05:41|366              |1.5          |-73.965157      |40.759403      |-73.980194       |40.749165       |CRD         |6.5        |1.0      |0.5    |1.6       |0.0         |1375383575|1375383941|366     |22.17     |22.17   |\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+----------+----------+--------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_grid = udf(grid_cells_q1, FloatType())\n",
    "\n",
    "taxi_df_q1 = sample_df.withColumn(\"start_cell\", get_grid(sample_df.pickup_longitude, sample_df.pickup_latitude))\\\n",
    "    .withColumn(\"end_cell\", get_grid(sample_df.dropoff_longitude, sample_df.dropoff_latitude))\\\n",
    "    .filter(\n",
    "        col(\"start_cell\").isNotNull() & col(\"end_cell\").isNotNull()  # Filter out trips outside of the grid\n",
    "    )\n",
    "\n",
    "taxi_df_q1.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIv-IUsW5R1O"
   },
   "source": [
    "### Query 1\n",
    "Frequent Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U31yCwN_5R1P",
    "outputId": "d7cdaaee-fb1e-4347-aef0-55818fbb2b7d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Cells for Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lat = 41.474937\n",
    "start_long = -74.913585\n",
    "cell_size = 0.022 # 250m to degrees for latitude (and longitude)\n",
    "\n",
    "def grid_cells_q2(point_long, point_lat):\n",
    "    \n",
    "    long = math.floor((point_long - start_long) / cell_size) + 1\n",
    "    lat = math.floor((start_lat - point_lat) / cell_size) + 1\n",
    "    \n",
    "    # Ensure the cell is within valid grid bounds (600x600)\n",
    "    if not (1 <= long <= 600 and 1 <= lat <= 600):\n",
    "        return None \n",
    "    \n",
    "    return float(f\"{long}.{lat}\") # Convert to X.X format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+----------+----------+--------+----------+--------+\n",
      "|medallion                       |hack_license                    |pickup_datetime    |dropoff_datetime   |trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|pickup_ts |dropoff_ts|duration|start_cell|end_cell|\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+----------+----------+--------+----------+--------+\n",
      "|64187531006B3D8B3831D91CD6AB3FD4|871C25EC0B8BF4BC8EF4CD3A79DD6BC8|2013-08-01 17:47:36|2013-08-01 19:05:40|4683             |18.0         |-74.005295      |40.750935      |-73.783058       |40.64394        |CRD         |52.0       |0.0      |0.5    |10.5      |0.0         |1375379256|1375383940|4684    |42.33     |52.38   |\n",
      "|B94FC976AA80935B3D978C872D8BDCA5|398ACB3ED20F01C7B015FD92199EF6FE|2013-08-01 19:00:45|2013-08-01 19:05:40|294              |0.7          |-73.975937      |40.749283      |-73.966896       |40.754253       |CRD         |5.0        |1.0      |0.5    |1.3       |0.0         |1375383645|1375383940|295     |43.33     |44.33   |\n",
      "|F3E2FC48C5A14BACE539216837A8AB86|ECB6E18FA26FB71E50B77E4E3265EE50|2013-08-01 18:57:33|2013-08-01 19:05:40|486              |0.8          |-73.9814        |40.763718      |-73.980293       |40.754337       |CRD         |6.5        |1.0      |0.5    |1.6       |0.0         |1375383453|1375383940|487     |43.33     |43.33   |\n",
      "|E79712363B74EB213BDE6F8FB1163FB4|85CBC591C44A5E763D4B9ECB11223732|2013-08-01 18:58:11|2013-08-01 19:05:41|449              |1.8          |-74.006783      |40.728691      |-74.003693       |40.750854       |CRD         |8.5        |1.0      |0.5    |2.0       |0.0         |1375383491|1375383941|450     |42.34     |42.33   |\n",
      "|FBB7B6B4099AD7F45A8AAC9A4A40A5E6|58057A0DC7DC000BA6A66DA13C87DD5D|2013-08-01 18:59:35|2013-08-01 19:05:41|366              |1.5          |-73.965157      |40.759403      |-73.980194       |40.749165       |CRD         |6.5        |1.0      |0.5    |1.6       |0.0         |1375383575|1375383941|366     |44.33     |43.33   |\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+----------+----------+--------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_grid2 = udf(grid_cells_q2, FloatType())\n",
    "\n",
    "taxi_df_q2 = sample_df.withColumn(\"start_cell\", get_grid2(sample_df.pickup_longitude, sample_df.pickup_latitude))\\\n",
    "    .withColumn(\"end_cell\", get_grid2(sample_df.dropoff_longitude, sample_df.dropoff_latitude))\\\n",
    "    .filter(\n",
    "        col(\"start_cell\").isNotNull() & col(\"end_cell\").isNotNull()  # Filter out trips outside of the grid\n",
    "    )\n",
    "\n",
    "taxi_df_q2.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAG1UbBE5R1P"
   },
   "source": [
    "### Query 2\n",
    "Profitable Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K7jwl2aoiZKk",
    "outputId": "c108e39e-4048-4671-a393-ccf3cd345d67"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+----------------+----------------+-----------------+----------+\n",
      "|              window|pickup_latitude|pickup_longitude|dropoff_latitude|dropoff_longitude|trip_count|\n",
      "+--------------------+---------------+----------------+----------------+-----------------+----------+\n",
      "|{2013-01-03 09:30...|      40.752472|      -73.938568|       40.752472|       -73.938568|         5|\n",
      "|{2013-01-01 13:00...|      40.744915|      -73.949043|       40.744915|       -73.949043|         5|\n",
      "|{2013-01-08 19:30...|      40.684944|       -73.98587|       40.684944|        -73.98587|         5|\n",
      "|{2013-01-10 19:00...|      40.752098|      -73.982376|       40.752098|       -73.982376|         5|\n",
      "|{2013-01-03 11:30...|      40.769669|       -73.95285|       40.769669|        -73.95285|         4|\n",
      "|{2013-01-01 18:00...|      40.755325|       -73.99646|       40.755325|        -73.99646|         4|\n",
      "|{2013-01-09 08:30...|      40.744915|      -73.949043|       40.744915|       -73.949043|         4|\n",
      "|{2013-01-05 11:00...|      40.762417|       -73.92395|       40.762417|        -73.92395|         4|\n",
      "|{2013-01-10 22:30...|      40.631966|      -73.977379|       40.631966|       -73.977379|         4|\n",
      "|{2013-01-03 23:00...|      40.746544|      -73.929001|       40.746544|       -73.929001|         4|\n",
      "+--------------------+---------------+----------------+----------------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, unix_timestamp, window, count, sum, expr, lit, udf\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType, StructType, StructField\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"DEBS_Taxi_Batch\").getOrCreate()\n",
    "\n",
    "# Define Schema for the CSV\n",
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"surcharge\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Read the CSV file\n",
    "taxi_df = spark.read.csv(\"input/sorted_data_sample.csv\", header=True, schema=schema)\n",
    "\n",
    "\n",
    "taxi_df = taxi_df.filter(\n",
    "    (regexp_extract(col(\"medallion\"), r\"^[a-fA-F0-9]{32}$\", 0) != \"\") &\n",
    "    (regexp_extract(col(\"hack_license\"), r\"^[a-fA-F0-9]{32}$\", 0) != \"\") &\n",
    "    (col(\"pickup_datetime\").isNotNull()) &\n",
    "    (col(\"dropoff_datetime\").isNotNull()) &               \n",
    "    (col(\"trip_distance\") > 0) &                    \n",
    "    (col(\"fare_amount\") > 0) &\n",
    "    (col(\"tip_amount\") >= 0)\n",
    ")\n",
    "\n",
    "# Convert timestamps to Unix format \n",
    "taxi_df = taxi_df.withColumn(\"pickup_ts\", unix_timestamp(\"pickup_datetime\")) \\\n",
    "    .withColumn(\"dropoff_ts\", unix_timestamp(\"dropoff_datetime\")) \\\n",
    "    .withColumn(\"duration\", col(\"dropoff_ts\") - col(\"pickup_ts\")) \\\n",
    "    .select(\"*\") \\\n",
    "    .dropna()  # Drop remaining null values\n",
    "\n",
    "# ------------------------------------ QUERY 0: Data Cleansing ------------------------------------ #\n",
    "taxi_df = taxi_df.filter((col(\"pickup_latitude\").between(40.5, 41.9)) &\n",
    "                         (col(\"pickup_longitude\").between(-74.3, -73.7)) &\n",
    "                         (col(\"dropoff_latitude\").between(40.5, 41.9)) &\n",
    "                         (col(\"dropoff_longitude\").between(-74.3, -73.7)) &\n",
    "                         (col(\"fare_amount\") > 0))\n",
    "\n",
    "# ------------------------------------ QUERY 1: Frequent Routes ------------------------------------ #\n",
    "route_counts = taxi_df.groupBy(\n",
    "    window(col(\"pickup_datetime\"), \"30 minutes\"),\n",
    "    \"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"\n",
    ").agg(count(\"*\").alias(\"trip_count\"))\n",
    "\n",
    "\n",
    "# Get Top 10 Routes\n",
    "top_routes = route_counts.orderBy(col(\"trip_count\").desc()).limit(10)\n",
    "\n",
    "# Show Query 1 results\n",
    "top_routes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- trip_time_in_secs: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- surcharge: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- pickup_ts: long (nullable = true)\n",
      " |-- dropoff_ts: long (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\nGlobalLimit 10\n+- LocalLimit 10\n   +- Sort [count#145L DESC NULLS LAST], true\n      +- Aggregate [pickup_latitude#7, pickup_longitude#6], [pickup_latitude#7, pickup_longitude#6, count(1) AS count#145L]\n         +- Aggregate [window#131-T3600000ms, pickup_latitude#7, pickup_longitude#6, dropoff_latitude#9, dropoff_longitude#8], [window#131-T3600000ms AS window#109-T3600000ms, pickup_latitude#7, pickup_longitude#6, dropoff_latitude#9, dropoff_longitude#8, count(1) AS trip_count#130L]\n            +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(pickup_datetime#2-T3600000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(pickup_datetime#2-T3600000ms, TimestampType, LongType) - 0) % 1800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(pickup_datetime#2-T3600000ms, TimestampType, LongType) - 0) % 1800000000) + 1800000000) ELSE ((precisetimestampconversion(pickup_datetime#2-T3600000ms, TimestampType, LongType) - 0) % 1800000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(pickup_datetime#2-T3600000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(pickup_datetime#2-T3600000ms, TimestampType, LongType) - 0) % 1800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(pickup_datetime#2-T3600000ms, TimestampType, LongType) - 0) % 1800000000) + 1800000000) ELSE ((precisetimestampconversion(pickup_datetime#2-T3600000ms, TimestampType, LongType) - 0) % 1800000000) END) - 0) + 1800000000), LongType, TimestampType))) AS window#131-T3600000ms, medallion#0, hack_license#1, pickup_datetime#2-T3600000ms, dropoff_datetime#3, trip_time_in_secs#4, trip_distance#5, pickup_longitude#6, pickup_latitude#7, dropoff_longitude#8, dropoff_latitude#9, payment_type#10, fare_amount#11, surcharge#12, mta_tax#13, tip_amount#14, tolls_amount#15, pickup_ts#33L, dropoff_ts#51L, duration#70L]\n               +- Filter isnotnull(pickup_datetime#2-T3600000ms)\n                  +- EventTimeWatermark pickup_datetime#2: timestamp, 1 hours\n                     +- Filter ((((((pickup_latitude#7 >= 40.5) AND (pickup_latitude#7 <= 41.9)) AND ((pickup_longitude#6 >= -74.3) AND (pickup_longitude#6 <= -73.7))) AND ((dropoff_latitude#9 >= 40.5) AND (dropoff_latitude#9 <= 41.9))) AND ((dropoff_longitude#8 >= -74.3) AND (dropoff_longitude#8 <= -73.7))) AND (fare_amount#11 > cast(0 as double)))\n                        +- Filter atleastnnonnulls(19, medallion#0, hack_license#1, pickup_datetime#2, dropoff_datetime#3, trip_time_in_secs#4, trip_distance#5, pickup_longitude#6, pickup_latitude#7, dropoff_longitude#8, dropoff_latitude#9, payment_type#10, fare_amount#11, surcharge#12, mta_tax#13, tip_amount#14, tolls_amount#15, pickup_ts#33L, dropoff_ts#51L, duration#70L)\n                           +- Project [medallion#0, hack_license#1, pickup_datetime#2, dropoff_datetime#3, trip_time_in_secs#4, trip_distance#5, pickup_longitude#6, pickup_latitude#7, dropoff_longitude#8, dropoff_latitude#9, payment_type#10, fare_amount#11, surcharge#12, mta_tax#13, tip_amount#14, tolls_amount#15, pickup_ts#33L, dropoff_ts#51L, (dropoff_ts#51L - pickup_ts#33L) AS duration#70L]\n                              +- Project [medallion#0, hack_license#1, pickup_datetime#2, dropoff_datetime#3, trip_time_in_secs#4, trip_distance#5, pickup_longitude#6, pickup_latitude#7, dropoff_longitude#8, dropoff_latitude#9, payment_type#10, fare_amount#11, surcharge#12, mta_tax#13, tip_amount#14, tolls_amount#15, pickup_ts#33L, unix_timestamp(dropoff_datetime#3, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) AS dropoff_ts#51L]\n                                 +- Project [medallion#0, hack_license#1, pickup_datetime#2, dropoff_datetime#3, trip_time_in_secs#4, trip_distance#5, pickup_longitude#6, pickup_latitude#7, dropoff_longitude#8, dropoff_latitude#9, payment_type#10, fare_amount#11, surcharge#12, mta_tax#13, tip_amount#14, tolls_amount#15, unix_timestamp(pickup_datetime#2, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) AS pickup_ts#33L]\n                                    +- Filter ((((((NOT (regexp_extract(medallion#0, ^[a-fA-F0-9]{32}$, 0) = ) AND NOT (regexp_extract(hack_license#1, ^[a-fA-F0-9]{32}$, 0) = )) AND isnotnull(pickup_datetime#2)) AND isnotnull(dropoff_datetime#3)) AND (trip_distance#5 > cast(0 as double))) AND (fare_amount#11 > cast(0 as double))) AND (tip_amount#14 >= cast(0 as double)))\n                                       +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@d663220,csv,List(),Some(StructType(StructField(medallion,StringType,true),StructField(hack_license,StringType,true),StructField(pickup_datetime,TimestampType,true),StructField(dropoff_datetime,TimestampType,true),StructField(trip_time_in_secs,IntegerType,true),StructField(trip_distance,DoubleType,true),StructField(pickup_longitude,DoubleType,true),StructField(pickup_latitude,DoubleType,true),StructField(dropoff_longitude,DoubleType,true),StructField(dropoff_latitude,DoubleType,true),StructField(payment_type,StringType,true),StructField(fare_amount,DoubleType,true),StructField(surcharge,DoubleType,true),StructField(mta_tax,DoubleType,true),StructField(tip_amount,DoubleType,true),StructField(tolls_amount,DoubleType,true))),List(),None,Map(header -> true, path -> input/),None), FileSource[input/], [medallion#0, hack_license#1, pickup_datetime#2, dropoff_datetime#3, trip_time_in_secs#4, trip_distance#5, pickup_longitude#6, pickup_latitude#7, dropoff_longitude#8, dropoff_latitude#9, payment_type#10, fare_amount#11, surcharge#12, mta_tax#13, tip_amount#14, tolls_amount#15]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 87\u001b[0m\n\u001b[1;32m     77\u001b[0m taxi_df_stream_cleaned\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Test if data is available in the stream by showing a small sample\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# taxi_df_stream_cleaned.show(10)  # Uncomment to show some rows from the cleaned stream\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Set the output mode to \"append\" for real-time updates and trigger processing every 10 seconds\u001b[39;00m\n\u001b[1;32m     83\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[43mtop_routes_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconsole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessingTime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m10 seconds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 87\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Wait for the termination of the query\u001b[39;00m\n\u001b[1;32m     90\u001b[0m query\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/readwriter.py:1527\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\nGlobalLimit 10\n+- LocalLimit 10\n   +- Sort [count#145L DESC NULLS LAST], true\n      +- Aggregate [pickup_latitude#7, pickup_longitude#6], [pickup_latitude#7, pickup_longitude#6, count(1) AS count#145L]\n         +- Aggregate [window#131-T3600000ms, pickup_latitude#7, pickup_longitude#6, dropoff_latitude#9, dropoff_longitude#8], [window#131-T3600000ms AS window#109-T3600000ms, pickup_latitude#7, pickup_longitude#6, dropoff_latitude#9, dropoff_longitude#8, count(1) AS trip_count#130L]\n            +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(pickup_datetime#2-T3600000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(pickup_datetime#2-T3600000ms, TimestampType, LongType) - 0) % 1800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(pickup_datetime#2-T3600000ms, TimestampType, LongType) - 0) % 1800000000) + 1800000000) ELSE ((precisetimestampconversion(pickup_datetime#2-T3600000ms, TimestampType, LongType) - 0) % 1800000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(pickup_datetime#2-T3600000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(pickup_datetime#2-T3600000ms, TimestampType, LongType) - 0) % 1800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(pickup_datetime#2-T3600000ms, TimestampType, LongType) - 0) % 1800000000) + 1800000000) ELSE ((precisetimestampconversion(pickup_datetime#2-T3600000ms, TimestampType, LongType) - 0) % 1800000000) END) - 0) + 1800000000), LongType, TimestampType))) AS window#131-T3600000ms, medallion#0, hack_license#1, pickup_datetime#2-T3600000ms, dropoff_datetime#3, trip_time_in_secs#4, trip_distance#5, pickup_longitude#6, pickup_latitude#7, dropoff_longitude#8, dropoff_latitude#9, payment_type#10, fare_amount#11, surcharge#12, mta_tax#13, tip_amount#14, tolls_amount#15, pickup_ts#33L, dropoff_ts#51L, duration#70L]\n               +- Filter isnotnull(pickup_datetime#2-T3600000ms)\n                  +- EventTimeWatermark pickup_datetime#2: timestamp, 1 hours\n                     +- Filter ((((((pickup_latitude#7 >= 40.5) AND (pickup_latitude#7 <= 41.9)) AND ((pickup_longitude#6 >= -74.3) AND (pickup_longitude#6 <= -73.7))) AND ((dropoff_latitude#9 >= 40.5) AND (dropoff_latitude#9 <= 41.9))) AND ((dropoff_longitude#8 >= -74.3) AND (dropoff_longitude#8 <= -73.7))) AND (fare_amount#11 > cast(0 as double)))\n                        +- Filter atleastnnonnulls(19, medallion#0, hack_license#1, pickup_datetime#2, dropoff_datetime#3, trip_time_in_secs#4, trip_distance#5, pickup_longitude#6, pickup_latitude#7, dropoff_longitude#8, dropoff_latitude#9, payment_type#10, fare_amount#11, surcharge#12, mta_tax#13, tip_amount#14, tolls_amount#15, pickup_ts#33L, dropoff_ts#51L, duration#70L)\n                           +- Project [medallion#0, hack_license#1, pickup_datetime#2, dropoff_datetime#3, trip_time_in_secs#4, trip_distance#5, pickup_longitude#6, pickup_latitude#7, dropoff_longitude#8, dropoff_latitude#9, payment_type#10, fare_amount#11, surcharge#12, mta_tax#13, tip_amount#14, tolls_amount#15, pickup_ts#33L, dropoff_ts#51L, (dropoff_ts#51L - pickup_ts#33L) AS duration#70L]\n                              +- Project [medallion#0, hack_license#1, pickup_datetime#2, dropoff_datetime#3, trip_time_in_secs#4, trip_distance#5, pickup_longitude#6, pickup_latitude#7, dropoff_longitude#8, dropoff_latitude#9, payment_type#10, fare_amount#11, surcharge#12, mta_tax#13, tip_amount#14, tolls_amount#15, pickup_ts#33L, unix_timestamp(dropoff_datetime#3, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) AS dropoff_ts#51L]\n                                 +- Project [medallion#0, hack_license#1, pickup_datetime#2, dropoff_datetime#3, trip_time_in_secs#4, trip_distance#5, pickup_longitude#6, pickup_latitude#7, dropoff_longitude#8, dropoff_latitude#9, payment_type#10, fare_amount#11, surcharge#12, mta_tax#13, tip_amount#14, tolls_amount#15, unix_timestamp(pickup_datetime#2, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) AS pickup_ts#33L]\n                                    +- Filter ((((((NOT (regexp_extract(medallion#0, ^[a-fA-F0-9]{32}$, 0) = ) AND NOT (regexp_extract(hack_license#1, ^[a-fA-F0-9]{32}$, 0) = )) AND isnotnull(pickup_datetime#2)) AND isnotnull(dropoff_datetime#3)) AND (trip_distance#5 > cast(0 as double))) AND (fare_amount#11 > cast(0 as double))) AND (tip_amount#14 >= cast(0 as double)))\n                                       +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@d663220,csv,List(),Some(StructType(StructField(medallion,StringType,true),StructField(hack_license,StringType,true),StructField(pickup_datetime,TimestampType,true),StructField(dropoff_datetime,TimestampType,true),StructField(trip_time_in_secs,IntegerType,true),StructField(trip_distance,DoubleType,true),StructField(pickup_longitude,DoubleType,true),StructField(pickup_latitude,DoubleType,true),StructField(dropoff_longitude,DoubleType,true),StructField(dropoff_latitude,DoubleType,true),StructField(payment_type,StringType,true),StructField(fare_amount,DoubleType,true),StructField(surcharge,DoubleType,true),StructField(mta_tax,DoubleType,true),StructField(tip_amount,DoubleType,true),StructField(tolls_amount,DoubleType,true))),List(),None,Map(header -> true, path -> input/),None), FileSource[input/], [medallion#0, hack_license#1, pickup_datetime#2, dropoff_datetime#3, trip_time_in_secs#4, trip_distance#5, pickup_longitude#6, pickup_latitude#7, dropoff_longitude#8, dropoff_latitude#9, payment_type#10, fare_amount#11, surcharge#12, mta_tax#13, tip_amount#14, tolls_amount#15]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, unix_timestamp, window, count, expr\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType, StructType, StructField, TimestampType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"DEBS_Taxi_Stream\").getOrCreate()\n",
    "\n",
    "# Define Schema for the CSV\n",
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"surcharge\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Read from CSV directory as a stream (replace \"input/directory\" with your path)\n",
    "taxi_df_stream = spark.readStream.option(\"header\", \"true\").schema(schema).csv(\"input/\")\n",
    "\n",
    "# Cleanse the data\n",
    "taxi_df_stream_cleaned = taxi_df_stream.filter(\n",
    "    (expr(\"regexp_extract(medallion, '^[a-fA-F0-9]{32}$', 0) != ''\")) &\n",
    "    (expr(\"regexp_extract(hack_license, '^[a-fA-F0-9]{32}$', 0) != ''\")) &\n",
    "    (col(\"pickup_datetime\").isNotNull()) &\n",
    "    (col(\"dropoff_datetime\").isNotNull()) &               \n",
    "    (col(\"trip_distance\") > 0) &                    \n",
    "    (col(\"fare_amount\") > 0) &\n",
    "    (col(\"tip_amount\") >= 0)\n",
    ")\n",
    "\n",
    "# Convert timestamps to Unix format \n",
    "taxi_df_stream_cleaned = taxi_df_stream_cleaned.withColumn(\"pickup_ts\", unix_timestamp(\"pickup_datetime\")) \\\n",
    "    .withColumn(\"dropoff_ts\", unix_timestamp(\"dropoff_datetime\")) \\\n",
    "    .withColumn(\"duration\", col(\"dropoff_ts\") - col(\"pickup_ts\")) \\\n",
    "    .dropna()  # Drop remaining null values\n",
    "\n",
    "# ------------------------------------ QUERY 0: Data Cleansing ------------------------------------ #\n",
    "taxi_df_stream_cleaned = taxi_df_stream_cleaned.filter(\n",
    "    (col(\"pickup_latitude\").between(40.5, 41.9)) &\n",
    "    (col(\"pickup_longitude\").between(-74.3, -73.7)) &\n",
    "    (col(\"dropoff_latitude\").between(40.5, 41.9)) &\n",
    "    (col(\"dropoff_longitude\").between(-74.3, -73.7)) &\n",
    "    (col(\"fare_amount\") > 0)\n",
    ")\n",
    "\n",
    "# ------------------------------------ QUERY 1: Frequent Routes ------------------------------------ #\n",
    "# Define a watermark and perform aggregation over the window\n",
    "taxi_df_stream_cleaned_with_watermark = taxi_df_stream_cleaned.withWatermark(\"pickup_datetime\", \"1 hour\")\n",
    "\n",
    "# Group by a 30-minute window and calculate trip counts\n",
    "route_counts_stream = taxi_df_stream_cleaned_with_watermark.groupBy(\n",
    "    window(col(\"pickup_datetime\"), \"30 minutes\"),\n",
    "    \"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"\n",
    ").agg(count(\"*\").alias(\"trip_count\"))\n",
    "\n",
    "# Aggregate by the window and the route\n",
    "route_counts_stream = route_counts_stream.groupBy(\"pickup_latitude\", \"pickup_longitude\").count()\n",
    "\n",
    "# Now, you can sort since it's an aggregated stream\n",
    "top_routes_stream = route_counts_stream.orderBy(col(\"count\").desc()).limit(10)\n",
    "\n",
    "# Set the configuration to disable the correctness check\n",
    "spark.conf.set(\"spark.sql.streaming.statefulOperator.checkCorrectness.enabled\", \"false\")\n",
    "\n",
    "# Check the schema of the cleaned data (just to ensure it's being processed correctly)\n",
    "taxi_df_stream_cleaned.printSchema()\n",
    "\n",
    "# Test if data is available in the stream by showing a small sample\n",
    "# taxi_df_stream_cleaned.show(10)  # Uncomment to show some rows from the cleaned stream\n",
    "\n",
    "# Set the output mode to \"append\" for real-time updates and trigger processing every 10 seconds\n",
    "query = top_routes_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the termination of the query\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TaxiTripsStream\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the path to the input data (replace with your actual file path)\n",
    "input_path = \"input/\"  # Modify this to point to your actual file or directory\n",
    "\n",
    "# Read the streaming data (assuming the data is in CSV format)\n",
    "taxi_df_stream = spark.readStream \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(\"medallion STRING, hack_license STRING, pickup_datetime TIMESTAMP, dropoff_datetime TIMESTAMP, \"\n",
    "            \"trip_time_in_secs INT, trip_distance DOUBLE, pickup_longitude DOUBLE, pickup_latitude DOUBLE, \"\n",
    "            \"dropoff_longitude DOUBLE, dropoff_latitude DOUBLE, payment_type STRING, fare_amount DOUBLE, \"\n",
    "            \"surcharge DOUBLE, mta_tax DOUBLE, tip_amount DOUBLE, tolls_amount DOUBLE, pickup_ts LONG, \"\n",
    "            \"dropoff_ts LONG, duration LONG\") \\\n",
    "    .csv(input_path)\n",
    "\n",
    "# Clean and filter the streaming DataFrame as needed\n",
    "taxi_df_stream_cleaned = taxi_df_stream.filter(\n",
    "    \"pickup_datetime IS NOT NULL AND dropoff_datetime IS NOT NULL AND trip_distance > 0 AND fare_amount > 0 AND tip_amount >= 0\"\n",
    ")\n",
    "\n",
    "# Apply watermark on the 'pickup_datetime' column with a 1-hour delay\n",
    "taxi_df_stream_cleaned_with_watermark = taxi_df_stream_cleaned \\\n",
    "    .withWatermark(\"pickup_datetime\", \"1 hour\")\n",
    "\n",
    "# Perform aggregation with windowing to group trips by pickup location and 1-hour time window\n",
    "top_routes_stream = taxi_df_stream_cleaned_with_watermark \\\n",
    "    .groupBy(\n",
    "        window(\"pickup_datetime\", \"1 hour\"),\n",
    "        \"pickup_latitude\",\n",
    "        \"pickup_longitude\"\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "# Output the result to the console (for demonstration purposes)\n",
    "query = top_routes_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the termination of the query\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------+------------------------+---------------------+\n",
      "|profitable_cell_id|empty_taxis_in_cell_id|median_profit_in_cell_id|profitability_of_cell|\n",
      "+------------------+----------------------+------------------------+---------------------+\n",
      "|           274.-57|                     1|                   200.0|                100.0|\n",
      "|          290.-286|                     1|                   170.0|                 85.0|\n",
      "|          241.-294|                     1|                   163.2|                 81.6|\n",
      "|          400.-229|                     1|                   154.0|                 77.0|\n",
      "|          272.-328|                     1|                   152.5|                76.25|\n",
      "|          352.-372|                     1|                   145.0|                 72.5|\n",
      "|          410.-158|                     1|                   140.0|                 70.0|\n",
      "|          356.-140|                     1|                   138.0|                 69.0|\n",
      "|          389.-126|                     1|                   136.0|                 68.0|\n",
      "|          265.-335|                     1|                   120.0|                 60.0|\n",
      "+------------------+----------------------+------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------ QUERY 2: Profitable Areas ------------------------------------ #\n",
    "# UDF to compute grid cell\n",
    "def compute_grid_cell(latitude, longitude, lat_start=41.474937, lon_start=-74.913585, cell_size=0.002247):\n",
    "    if latitude is None or longitude is None:\n",
    "        return None\n",
    "    cell_x = 1 + int((longitude - lon_start) / 0.00294)\n",
    "    cell_y = 1 + int((latitude - lat_start) / 0.002247)\n",
    "    return f\"{cell_x}.{cell_y}\"\n",
    "\n",
    "grid_udf = udf(compute_grid_cell, StringType())\n",
    "\n",
    "# Assign grid cells\n",
    "taxi_df = taxi_df.withColumn(\"pickup_cell\", grid_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\"))) \\\n",
    "                 .withColumn(\"dropoff_cell\", grid_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\")))\n",
    "\n",
    "\n",
    "# Compute Median Profit Per Cell\n",
    "profit_df = taxi_df.groupBy(\"pickup_cell\").agg(expr(\"percentile_approx(fare_amount + tip_amount, 0.5)\").alias(\"median_profit\"))\n",
    "\n",
    "# Count Empty Taxis Per Cell\n",
    "empty_taxis_df = taxi_df.groupBy(\"dropoff_cell\").agg(count(\"*\").alias(\"empty_taxis\"))\n",
    "\n",
    "# Compute Profitability and Get Top 10 Profitable Cells\n",
    "profitability_df = profit_df.join(empty_taxis_df, profit_df.pickup_cell == empty_taxis_df.dropoff_cell, \"left_outer\") \\\n",
    "    .withColumn(\"profitability\", col(\"median_profit\") / (col(\"empty_taxis\") + lit(1)))\n",
    "\n",
    "top_profitable_cells = profitability_df.orderBy(col(\"profitability\").desc()).limit(10)\n",
    "\n",
    "# Show Query 2 results\n",
    "top_profitable_cells.selectExpr(\n",
    "    \"pickup_cell as profitable_cell_id\",\n",
    "    \"empty_taxis as empty_taxis_in_cell_id\",\n",
    "    \"median_profit as median_profit_in_cell_id\",\n",
    "    \"profitability as profitability_of_cell\"\n",
    ").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Practice session - Dataframe",
   "notebookOrigID": 1061204080530756,
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
