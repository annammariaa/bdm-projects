{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1w9AYcq5R1L"
   },
   "source": [
    "# Big Data Management Project 2:\n",
    "## DESB GRAND CHALLENGE 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpzgMHds5R1M",
    "outputId": "c2717441-381b-48f5-8e5b-2c43f3ebd6d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shapely\n",
      "  Using cached shapely-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.11/site-packages (from shapely) (1.26.4)\n",
      "Using cached shapely-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "Installing collected packages: shapely\n",
      "Successfully installed shapely-2.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, unix_timestamp, regexp_extract, col, lag, avg, lead, count, sum as spark_sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.5.3)\n",
      "Collecting pyspark\n",
      "  Using cached pyspark-3.5.5-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.5.3\n",
      "    Can't uninstall 'pyspark'. No files were found to uninstall.\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_NyMxyS5R1O"
   },
   "source": [
    "### Query 0\n",
    "Data Cleansing and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OYAFRgWI5R1O",
    "outputId": "cc41f6c7-b5d7-4ad4-9050-8ac6ca454008",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time() # To see the time it takes to execute data transformations\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('BDM_Project2') \\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "                    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# Defining the schema for faster reading of data\n",
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"passenger_count\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"surcharge\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Read the streaming data\n",
    "taxi_df_og = (\n",
    "    spark.readStream\n",
    "    .option(\"maxFilesPerTrigger\", 1)  # Read one file at a time to avoid overwhelming the system\n",
    "    .option(\"header\", True)\n",
    "    .schema(schema)\n",
    "    .csv(\"input/\")  # Streaming from directory\n",
    ")\n",
    "\n",
    "# Removing the trips with 0 passengers\n",
    "# Transforming the data \n",
    "taxi_df = taxi_df_og.filter(\n",
    "    (regexp_extract(col(\"medallion\"), r\"^[a-fA-F0-9]{32}$\", 0) != \"\") &\n",
    "    (regexp_extract(col(\"hack_license\"), r\"^[a-fA-F0-9]{32}$\", 0) != \"\") &\n",
    "    (col(\"pickup_datetime\").isNotNull()) &\n",
    "    (col(\"dropoff_datetime\").isNotNull()) &\n",
    "    (col(\"passenger_count\") > 0) &                 \n",
    "    (col(\"trip_distance\") > 0) &                    \n",
    "    (col(\"fare_amount\") > 0) &\n",
    "    (col(\"tip_amount\") > 0)\n",
    ")\n",
    "\n",
    "# Convert timestamps to Unix format \n",
    "taxi_df = taxi_df.withColumn(\"pickup_ts\", unix_timestamp(\"pickup_datetime\")) \\\n",
    "    .withColumn(\"dropoff_ts\", unix_timestamp(\"dropoff_datetime\")) \\\n",
    "    .withColumn(\"duration\", col(\"dropoff_ts\") - col(\"pickup_ts\")) \\\n",
    "    .filter(\n",
    "        (col(\"duration\") > 0) & (col(\"duration\") <= 4 * 60 * 60)) \\\n",
    "    .select(\"*\") \\\n",
    "    .dropna()  # Drop remaining null values\n",
    "\n",
    "# Write the output to Parquet files in a directory\n",
    "query = taxi_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"output/\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint/\") \\\n",
    "    .trigger(processingTime=\"1000 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "# Await termination to keep the stream running\n",
    "query.awaitTermination(60000)  # Wait for 60 seconds or until stopped\n",
    "\n",
    "\n",
    "#optional\n",
    "#original_count = taxi_df_og.count()\n",
    "#filtered_count = taxi_df.count()\n",
    "#filtered_out_count = original_count - filtered_count\n",
    "\n",
    "#print(f\"Original count: {original_count}\") \n",
    "#print(f\"Filtered count: {filtered_count}\")\n",
    "#print(f\"Rows filtered out: {filtered_out_count}\")\n",
    "print(\"Execution time\", time.time() - start_time)\n",
    "\n",
    "#optional\n",
    "#original_count = taxi_df_og.count()\n",
    "#filtered_count = taxi_df.count()\n",
    "#filtered_out_count = original_count - filtered_count\n",
    "\n",
    "#print(f\"Original count: {original_count}\") \n",
    "#print(f\"Filtered count: {filtered_count}\")\n",
    "#print(f\"Rows filtered out: {filtered_out_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.streaming.query.StreamingQuery object at 0x7f50b64d4950>\n"
     ]
    }
   ],
   "source": [
    "query = taxi_df.limit(5) \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"1000 seconds\") \\\n",
    "    .start()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
      "None\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Example to check query status and progress\n",
    "print(query.status)  # Prints the status of the query\n",
    "print(query.lastProgress)  # Prints the last progress made by the query\n",
    "print(query.isActive)  # Checks if the query is still active\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small count: 7516760\n"
     ]
    }
   ],
   "source": [
    "taxi_df_small = taxi_df.sample(fraction=0.0833)  # 1GB out of 12GB\n",
    "small_count = taxi_df_small.count()\n",
    "print(f\"Small count: {small_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIv-IUsW5R1O"
   },
   "source": [
    "### Query 1\n",
    "Frequent Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U31yCwN_5R1P",
    "outputId": "d7cdaaee-fb1e-4347-aef0-55818fbb2b7d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAG1UbBE5R1P"
   },
   "source": [
    "### Query 2\n",
    "Profitable Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K7jwl2aoiZKk",
    "outputId": "c108e39e-4048-4671-a393-ccf3cd345d67"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Practice session - Dataframe",
   "notebookOrigID": 1061204080530756,
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
