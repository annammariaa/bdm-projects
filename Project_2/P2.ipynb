{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1w9AYcq5R1L"
   },
   "source": [
    "# Big Data Management Project 2:\n",
    "## DESB GRAND CHALLENGE 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpzgMHds5R1M",
    "outputId": "c2717441-381b-48f5-8e5b-2c43f3ebd6d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shapely\n",
      "  Using cached shapely-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.11/site-packages (from shapely) (1.26.4)\n",
      "Using cached shapely-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "Installing collected packages: shapely\n",
      "Successfully installed shapely-2.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, unix_timestamp, regexp_extract, col, lag, avg, lead, count, sum as spark_sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.5.3)\n",
      "Collecting pyspark\n",
      "  Using cached pyspark-3.5.5-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.5.3\n",
      "    Can't uninstall 'pyspark'. No files were found to uninstall.\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_NyMxyS5R1O"
   },
   "source": [
    "### Query 0\n",
    "Data Cleansing and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time 1344.0212268829346\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('BDM_Project2') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for taxi trips\n",
    "taxi_schema = \"\"\"\n",
    "    medallion STRING, \n",
    "    hack_license STRING, \n",
    "    pickup_datetime TIMESTAMP, \n",
    "    dropoff_datetime TIMESTAMP, \n",
    "    trip_time_in_secs INT, \n",
    "    trip_distance DOUBLE, \n",
    "    pickup_longitude DOUBLE, \n",
    "    pickup_latitude DOUBLE, \n",
    "    dropoff_longitude DOUBLE, \n",
    "    dropoff_latitude DOUBLE, \n",
    "    payment_type STRING, \n",
    "    fare_amount DOUBLE, \n",
    "    surcharge DOUBLE, \n",
    "    mta_tax DOUBLE, \n",
    "    tip_amount DOUBLE, \n",
    "    tolls_amount DOUBLE\n",
    "\"\"\"\n",
    "\n",
    "taxi_df_og = (\n",
    "    spark.readStream\n",
    "    .schema(taxi_schema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)  # Read one file at a time\n",
    "    .option(\"header\", True)  # CSV files usually have headers\n",
    "    .csv(\"input/\")  # Change to CSV directory\n",
    ")\n",
    "\n",
    "# Removing the trips with 0 passengers\n",
    "# Transforming the data \n",
    "taxi_df = taxi_df_og.filter(\n",
    "    (regexp_extract(col(\"medallion\"), r\"^[a-fA-F0-9]{32}$\", 0) != \"\") &\n",
    "    (regexp_extract(col(\"hack_license\"), r\"^[a-fA-F0-9]{32}$\", 0) != \"\") &\n",
    "    (col(\"pickup_datetime\").isNotNull()) &\n",
    "    (col(\"dropoff_datetime\").isNotNull()) &               \n",
    "    (col(\"trip_distance\") > 0) &                    \n",
    "    (col(\"fare_amount\") > 0) &\n",
    "    (col(\"tip_amount\") > 0)\n",
    ")\n",
    "\n",
    "# Convert timestamps to Unix format \n",
    "taxi_df = taxi_df.withColumn(\"pickup_ts\", unix_timestamp(\"pickup_datetime\")) \\\n",
    "    .withColumn(\"dropoff_ts\", unix_timestamp(\"dropoff_datetime\")) \\\n",
    "    .withColumn(\"duration\", col(\"dropoff_ts\") - col(\"pickup_ts\")) \\\n",
    "    .select(\"*\") \\\n",
    "    .dropna()  # Drop remaining null values\n",
    "\n",
    "# Start the streaming query with trigger(once=True) to process data once and stop\n",
    "query = (\n",
    "    taxi_df.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", \"output/preprocessed_data\")\n",
    "    .option(\"checkpointLocation\", \"output/checkpoint\")\n",
    "    .trigger(once=True)  \n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "print(\"Execution time\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+----------+----------+--------+\n",
      "|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount| pickup_ts|dropoff_ts|duration|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+----------+----------+--------+\n",
      "|64187531006B3D8B3...|871C25EC0B8BF4BC8...|2013-08-01 17:47:36|2013-08-01 19:05:40|             4683|         18.0|      -74.005295|      40.750935|       -73.783058|        40.64394|         CRD|       52.0|      0.0|    0.5|      10.5|         0.0|1375379256|1375383940|    4684|\n",
      "|B94FC976AA80935B3...|398ACB3ED20F01C7B...|2013-08-01 19:00:45|2013-08-01 19:05:40|              294|          0.7|      -73.975937|      40.749283|       -73.966896|       40.754253|         CRD|        5.0|      1.0|    0.5|       1.3|         0.0|1375383645|1375383940|     295|\n",
      "|F3E2FC48C5A14BACE...|ECB6E18FA26FB71E5...|2013-08-01 18:57:33|2013-08-01 19:05:40|              486|          0.8|        -73.9814|      40.763718|       -73.980293|       40.754337|         CRD|        6.5|      1.0|    0.5|       1.6|         0.0|1375383453|1375383940|     487|\n",
      "|E79712363B74EB213...|85CBC591C44A5E763...|2013-08-01 18:58:11|2013-08-01 19:05:41|              449|          1.8|      -74.006783|      40.728691|       -74.003693|       40.750854|         CRD|        8.5|      1.0|    0.5|       2.0|         0.0|1375383491|1375383941|     450|\n",
      "|FBB7B6B4099AD7F45...|58057A0DC7DC000BA...|2013-08-01 18:59:35|2013-08-01 19:05:41|              366|          1.5|      -73.965157|      40.759403|       -73.980194|       40.749165|         CRD|        6.5|      1.0|    0.5|       1.6|         0.0|1375383575|1375383941|     366|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+----------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_df = spark.read.parquet(\"output/preprocessed_data\")\n",
    "output_df.show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIv-IUsW5R1O"
   },
   "source": [
    "### Query 1\n",
    "Frequent Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U31yCwN_5R1P",
    "outputId": "d7cdaaee-fb1e-4347-aef0-55818fbb2b7d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAG1UbBE5R1P"
   },
   "source": [
    "### Query 2\n",
    "Profitable Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K7jwl2aoiZKk",
    "outputId": "c108e39e-4048-4671-a393-ccf3cd345d67"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Practice session - Dataframe",
   "notebookOrigID": 1061204080530756,
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
