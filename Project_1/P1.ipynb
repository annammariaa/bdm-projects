{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Management Project 1:\n",
    "## Analyzing New York City Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shapely\n",
      "  Downloading shapely-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.11/site-packages (from shapely) (1.26.4)\n",
      "Downloading shapely-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m6.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: shapely\n",
      "Successfully installed shapely-2.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, unix_timestamp, col, lag, avg, lead, sum as spark_sum\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import json\n",
    "\n",
    "from shapely.geometry import shape, Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "                    .appName('BDM_Project1')\n",
    "                    .enableHiveSupport()  # Enables Hive support, persistent Hive metastore\n",
    "                    .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYC Borough Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input/nyc-boroughs.geojson') as f:\n",
    "    geo_data = json.load(f)\n",
    "\n",
    "# Broadcasting data to workers\n",
    "broadcast_geo_data = spark.sparkContext.broadcast(geo_data)\n",
    "\n",
    "# TODO: is using a dictionary fine? \"dataframe can be created out of it\"\n",
    "# Creating a dictionary of borough codes and polygons within the borough\n",
    "polygons = {}\n",
    "b_names = {} # borough names by code\n",
    "\n",
    "for feature in broadcast_geo_data.value['features']:\n",
    "    \n",
    "    code = feature['properties']['boroughCode']\n",
    "    name = feature['properties']['borough']\n",
    "\n",
    "    if code not in polygons:\n",
    "        polygons[code] = []\n",
    "        b_names[code] = name\n",
    "    \n",
    "    polygons[code].append(shape(feature['geometry']))\n",
    "\n",
    "# Sorting borough polygons by area\n",
    "for code in polygons: \n",
    "    polygons[code] = sorted(\n",
    "        polygons[code], key=lambda x: x.area, reverse=True\n",
    "    )\n",
    "\n",
    "# also sort boroughs by total area?\n",
    "#borough_total_areas = {code: sum(poly.area for poly in polys) for code, polys in polygons.items()}\n",
    "#sorted_boroughs = sorted(borough_total_areas.keys(), key=lambda x: borough_total_areas[x], reverse=True)\n",
    "#polygons = {code: polygons[code] for code in sorted_boroughs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF: longitude, latitude -> borough\n",
    "def get_borough(long, lat):\n",
    "    point = Point(long, lat)\n",
    "    \n",
    "    for code, pols in polygons.items():\n",
    "        for polygon in pols:\n",
    "            if polygon.contains(point):\n",
    "                return code\n",
    "    \n",
    "    return None\n",
    "\n",
    "get_borough_udf = udf(get_borough, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYC Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---------------+----------------+---------------+----------------+-----------------+----------------+----------+----------+--------+--------------+---------------+\n",
      "|hack_license                    |pickup_latitude|pickup_longitude|pickup_datetime|dropoff_latitude|dropoff_longitude|dropoff_datetime|pickup_ts |dropoff_ts|duration|pickup_borough|dropoff_borough|\n",
      "+--------------------------------+---------------+----------------+---------------+----------------+-----------------+----------------+----------+----------+--------+--------------+---------------+\n",
      "|BA96DE419E711691B9445D6A6307C170|40.757977      |-73.978165      |01-01-13 15:11 |40.751171       |-73.989838       |01-01-13 15:18  |1357053060|1357053480|420     |1             |1              |\n",
      "|9FD8F69F0804BDB5549F40E9DA1BE472|40.731781      |-74.006683      |06-01-13 00:18 |40.75066        |-73.994499       |06-01-13 00:22  |1357431480|1357431720|240     |1             |1              |\n",
      "|9FD8F69F0804BDB5549F40E9DA1BE472|40.73777       |-74.004707      |05-01-13 18:49 |40.726002       |-74.009834       |05-01-13 18:54  |1357411740|1357412040|300     |1             |1              |\n",
      "+--------------------------------+---------------+----------------+---------------+----------------+-----------------+----------------+----------+----------+--------+--------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_df = (spark.read\n",
    "             .option(\"sep\", \",\")\n",
    "             .option(\"header\", True)\n",
    "             .option(\"inferSchema\", True)\n",
    "             .csv(\"input/Sample NYC Data.csv\")\n",
    "            )\n",
    "\n",
    "# Selecting only necessary columns\n",
    "taxi_df = taxi_df.select(\n",
    "    \"hack_license\",\n",
    "    \"pickup_latitude\",\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_datetime\",\n",
    "    \"dropoff_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_datetime\" \n",
    ")\n",
    "\n",
    "# Converting datetime to unix timestamp (seconds)\n",
    "taxi_df = taxi_df.withColumn(\n",
    "    \"pickup_ts\", unix_timestamp(\"pickup_datetime\", \"dd-MM-yy HH:mm\")\n",
    ").withColumn(\n",
    "    \"dropoff_ts\", unix_timestamp(\"dropoff_datetime\", \"dd-MM-yy HH:mm\")\n",
    ")\n",
    "\n",
    "# Calculating ride duration (seconds)\n",
    "taxi_df = taxi_df.withColumn(\n",
    "    \"duration\", (taxi_df[\"dropoff_ts\"] - taxi_df[\"pickup_ts\"])\n",
    ")\n",
    "\n",
    "# Filtering out rides longer than 4h or with negative duration\n",
    "taxi_df = taxi_df.filter((taxi_df[\"duration\"] > 0) & (taxi_df[\"duration\"] <= 4 * 60 * 60))\n",
    "\n",
    "# Add pick up and drop off boroughs to taxi data\n",
    "taxi_df = taxi_df.withColumn(\n",
    "    \"pickup_borough\", get_borough_udf(\"pickup_longitude\", \"pickup_latitude\")\n",
    ").withColumn(\n",
    "    \"dropoff_borough\", get_borough_udf(\"dropoff_longitude\", \"dropoff_latitude\")\n",
    ")\n",
    "taxi_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1\n",
    "Utilization: idle time per taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|        hack_license|total_idle_time|\n",
      "+--------------------+---------------+\n",
      "|001C8AAB90AEE49F3...|          12960|\n",
      "|0025133AD810DBE80...|           2400|\n",
      "|00447A6197DBB329F...|          13440|\n",
      "|006313464EC98A24B...|          31500|\n",
      "|007439EEDB510EF82...|           3240|\n",
      "|00927C48BA4C1B2B1...|          14460|\n",
      "|00AE05F56D451E89E...|          22200|\n",
      "|00B442110FA2D04A1...|          10680|\n",
      "|00BB5ECED533BF463...|          10380|\n",
      "|00BF52E4A8E6DBB01...|           9720|\n",
      "|00D0B6CE0ADA00D70...|           8940|\n",
      "|01060D63D29CE42C8...|           4200|\n",
      "|011707FD64AD1EBEA...|           8220|\n",
      "|011EB4B6E7DE7B08C...|           8220|\n",
      "|01202D837DD4454C7...|           8100|\n",
      "|0124A558E529199A8...|          19620|\n",
      "|013B64EE51129C462...|          14340|\n",
      "|014BB395ECEE67BEC...|           3360|\n",
      "|015B18400858D89EE...|           2700|\n",
      "|015D33FBAB8A7C5CE...|          18660|\n",
      "+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copy taxi_df dataframe\n",
    "taxi_copy = taxi_df.select(\"*\")\n",
    "\n",
    "# Ensure all trips of the same driver are together\n",
    "taxi_copy = taxi_copy.repartition(\"hack_license\")\n",
    "\n",
    "# Partition by driver and then order by pickup time with Window\n",
    "window_sp = Window.partitionBy(\"hack_license\").orderBy(\"pickup_ts\")\n",
    "\n",
    "# Compute previous dropoff time\n",
    "taxi_copy = taxi_copy.withColumn(\"prev_dropoff_ts\", lag(\"dropoff_ts\").over(window_sp))\n",
    "\n",
    "# Compute idle time\n",
    "taxi_copy = taxi_copy.withColumn(\"idle_time\", col(\"pickup_ts\") - col(\"prev_dropoff_ts\"))\n",
    "\n",
    "# Control that ride is not over 4h\n",
    "taxi_copy = taxi_copy.filter((col(\"idle_time\").isNotNull()) & (col(\"idle_time\") <= 14400))\n",
    "\n",
    "# Group by driver and sum idle time\n",
    "result = taxi_copy.groupBy(\"hack_license\").agg(spark_sum(\"idle_time\").alias(\"total_idle_time\"))\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2\n",
    "The average time it takes for a taxi to find its next fare(trip) per destination borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In minutes\n",
      "+---------------+------------------+\n",
      "|dropoff_borough|  avg_waiting_time|\n",
      "+---------------+------------------+\n",
      "|              1|30.784491026167967|\n",
      "|              3|111.74982332155477|\n",
      "|              5|167.88888888888889|\n",
      "|              4|100.00736478711163|\n",
      "|              2| 79.10493827160494|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(\"hack_license\").orderBy(\"pickup_ts\")\n",
    "\n",
    "#get the next pickup time\n",
    "next_pick_up = taxi_df.withColumn(\"next_pickup_ts\", lead(\"pickup_ts\").over(window_spec))\n",
    "\n",
    "wait_time = next_pick_up.withColumn(\"waiting_time\", (next_pick_up[\"next_pickup_ts\"] - next_pick_up[\"dropoff_ts\"])/60)\n",
    "wait_time = wait_time.na.drop(subset=[\"next_pickup_ts\"])\n",
    "wait_time = wait_time.filter(wait_time[\"dropoff_borough\"].isNotNull())\n",
    "\n",
    "result = wait_time.groupBy(\"dropoff_borough\").agg(avg(\"waiting_time\").alias(\"avg_waiting_time\"))\n",
    "print(\"In minutes\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 3\n",
    "The number of trips that started and ended within the same borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85944\n"
     ]
    }
   ],
   "source": [
    "sumOfTripsBoroughSame = taxi_df.filter(col(\"pickup_borough\") == col(\"dropoff_borough\")).count()\n",
    "print(sumOfTripsBoroughSame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 4\n",
    "The number of trips that started in one borough and ended in another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11431\n"
     ]
    }
   ],
   "source": [
    "sumOfTripsBoroughDifferent = taxi_df.filter(col(\"pickup_borough\") != col(\"dropoff_borough\")).count()\n",
    "print(sumOfTripsBoroughDifferent)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Practice session - Dataframe",
   "notebookOrigID": 1061204080530756,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}